{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d04643d-4607-4463-a842-91be11c11806",
   "metadata": {},
   "source": [
    "# Building an image classifier with a DNN using TensorFlow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6882db",
   "metadata": {},
   "source": [
    "## About This Session\n",
    "\n",
    "This lab is a guided walkthrough to build a _basic_ 'image classification' DNN model and train it on some data.\n",
    "\n",
    "As we build up our model we will build a familiarity with the various components and strategies for building such an ANN system.\n",
    "\n",
    "After we have been able to do this we will then save and load our now-trained model from disk and use it to make a prediction on a new image which isn't present in the original dataset.\n",
    "\n",
    "\n",
    "_Responsible: Robert Currie (<rob.currie@ed.ac.uk>)_\n",
    "\n",
    "### Notes on assessment\n",
    "* Try and calculate the answers to the exercises provided. If you are unable to complete the question, describe which approach you _would_ have taken to solve the problem\n",
    "* Code must be understandable and reproducible. Before grading the notebook kernel **may** be restarted and re-run, so make sure that your code can run from start to finish without any (unintentional) errors\n",
    "* If you are unsure on how to proceed please **ask one of the TAs** during the workshop\n",
    " - Notebooks should be submitted by **10am on Friday 13th October**\n",
    " - This notebook starts with problem 0. This is not assessed, but it is a helpful exercise to show some of the potential pitfalls of 'trusting algorithms' to do the work for us.\n",
    "\n",
    "* There are 10 sections involved in building a DNN using TensorFlow, trainig it on data and then using it to predict based on a new piece of data never seen by the model before.\n",
    "* There are 2 bonus sections covering 'additional training of a model' and 'the impact of training on the distribution of internal weights within a DNN'. These have bonus points if you manage to finish them, but the final mark is capped at 10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895060a6",
   "metadata": {},
   "source": [
    "##### Marking\n",
    "\n",
    "Marks for the different parts are shown below.\n",
    "\n",
    "* Questions are marked as starting with '**Q**' and can in principle be attempted at any time.\n",
    "* Problems are intended to be tackled in order, i.e. 1->9\n",
    "* If you jump in at problem 5 for instance you won't yet have a model to train without tackling the steps before.\n",
    "* There are bonus problems at the end to tackle but the maximum mark is 10/10\n",
    "\n",
    "| <p align='left'> Title                         | <p align='left'> Parts | <p align='left'> Number of marks |\n",
    "| ------------------------------------- | ----- | --- |\n",
    "| <p align='left'> 0. Dangers of Fitting                  | <p align='left'>  3  | <p align='left'> 0 |\n",
    "| <p align='left'> 1. Vizualizing the Data                | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 2. Formatting Input Data               | <p align='left'>  3  | <p align='left'> 1 |\n",
    "| <p align='left'> 3. Building a DNN model                | <p align='left'>  2  | <p align='left'> 1 |\n",
    "| <p align='left'> 4. Understanding our model             | <p align='left'>  5  | <p align='left'> 2 |\n",
    "| <p align='left'> 5. Training our model                  | <p align='left'>  2  | <p align='left'> 1 |\n",
    "| <p align='left'> 6. Understanding the training history  | <p align='left'>  3  | <p align='left'> 1 |\n",
    "| <p align='left'> 7. Making a Prediction                 | <p align='left'>  2  | <p align='left'> 1 |\n",
    "| <p align='left'> 8. Testing the model                   | <p align='left'>  2  | <p align='left'> 1 |\n",
    "| <p align='left'> 9. Making Predictions with new data    | <p align='left'>  2  | <p align='left'> 1 |\n",
    "| <p align='left'> **Bonus 1:** Analyzing Weights         | <p align='left'>  5  | <p align='left'> 1 |\n",
    "| <p align='left'> **Bonus 2:** Loading a model and optimizing it further | <p align='left'>  7  | <p align='left'> 1 |\n",
    "| <p align='left'> **Total** | | <p align='left'> max **10** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f685af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's keep our keras backend tensorflow quiet\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "# if you need to force tensorflow to use your CPU\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "# imports for array-handling and plotting\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Importing tensorflow so we can access anything later\n",
    "import tensorflow as tf\n",
    "\n",
    "# keras imports for importing our dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "# keras imports for building our neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "# keras import for manipulating some of our data\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# keras tools for loading an image from disk\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# keras tools for loading an ANN model from disk\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a90065f-9801-4619-ba6f-cb8fad34f6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec01e96",
   "metadata": {},
   "source": [
    "\n",
    "# Problem 0: Understanding Fitting\n",
    "\n",
    "### Problems when fitting (an interlude)\n",
    "\n",
    "When fitting to a dataset we trust that the algorithm that we're using will find the correct minima and that the fit hasn't become pronned to some problem such as vanishing gradients.\n",
    "\n",
    "Here we're going to demonstrate a quick example of problems that you can hit with a fit failing to converge.\n",
    "\n",
    "There are many tools/checks in place in most modern libraries to help fix common problems, but we will explicitly use the `SLSQP` method here which allows us to look at narrow ranges of a full function.\n",
    "\n",
    "`result = minimize(objective, pt, method='SLSQP', jac=derivative, bounds=[(...),])`\n",
    "\n",
    "The objective function of this fit has been provided it is the equation of the form `y=x^4-5x^2+0.1x` .\n",
    "\n",
    "This function has 3 points where the gradient is zero and has 2 minima, a global minima and a local minima.\n",
    "\n",
    "## 0.1 Provide the derivative function for this 1D method (this is the Jacobian)\n",
    "\n",
    "This is just a case of returning the correct value from the derivative method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "631f8cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from scipy) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective(x):\n",
    "  return x[0]**4 - 5*x[0]**2 + 0.1*x[0]\n",
    "\n",
    "# Part 1 complete the derivative function\n",
    "def derivative(x):\n",
    "  return 3*x[0]**3 - 10*x[0] + 0.1## FINISH ME ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e42b37",
   "metadata": {},
   "source": [
    "## 0.2 Run the minimize method over given ranges and identify the 3 'stable' points of the function\n",
    "\n",
    "Run the minimize method over these 3 ranges:\n",
    "\n",
    "| <p align='left'> Ranges |\n",
    "| :--- |\n",
    "| ` (0.1, 5.0)` |\n",
    "| ` (0.0, 5.0)`  |\n",
    "| `(-5.0, 5.0)` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf94f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = (5.,)\n",
    "ranges = [(0.1, 5.0),  (0.0, 5.0),  (-5.0, 5.0)]\n",
    "# result = minimize(objective, pt, method='SLSQP', jac=derivative, bounds=[(...),])\n",
    "# Part 2 perform 3 minimizations\n",
    "result_1 = minimize(objective, pt, method = 'SLSQP', jac = derivative, bounds = [ranges[0]]) # FINSIH ME #\n",
    "result_2 = minimize(objective, pt, method = 'SLSQP', jac = derivative, bounds = [ranges[1]])\n",
    "result_3 = minimize(objective, pt, method = 'SLSQP', jac = derivative, bounds = [ranges[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f176ac20-c564-46da-9a11-e42d7dbf20c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1, 5.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dd34b65",
   "metadata": {},
   "source": [
    "## 0.3 Identify the true minima of the function\n",
    "\n",
    "##### Hint:\n",
    "    \n",
    "Running `scipy.minimize` returns an object containing the fit result and a bunch of extra output.\n",
    "The value of `x` from a 'minimize' after it has finished running can be accessed via `result['x']` and the function value can be accessed via `result['fun']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f302c108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum 1: [1.649128]\n",
      "Minimum 2: [9.39076017e-09]\n",
      "Minimum 3: [-1.62910631]\n",
      "Result 1: -6.036852872507092\n",
      "Result 2: 9.3907557601621e-10\n",
      "Result 3: -6.389198522290075\n",
      "Best Result is:  -6.389198522290075\n"
     ]
    }
   ],
   "source": [
    "print('Minimum 1: {}'.format(result_1['x']))\n",
    "print('Minimum 2: {}'.format(result_2['x']))\n",
    "print('Minimum 3: {}'.format(result_3['x']))\n",
    "\n",
    "print('Result 1: {}'.format(result_1['fun']))\n",
    "print('Result 2: {}'.format(result_2['fun']))\n",
    "print('Result 3: {}'.format(result_3['fun']))\n",
    "\n",
    "list1 = [result_1['fun'], result_2['fun'], result_3['fun']]\n",
    "# Part 3 identify the true minima\n",
    "print('Best Result is: ', min(list1) )# FINISH ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3e317-6153-4d11-91d5-9bb157994ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe5e051f",
   "metadata": {},
   "source": [
    "# Load & Visualize the dataset\n",
    "\n",
    "#### Loading the data\n",
    "\n",
    "The data we're using comes from the well use mnist dataset which has been around since the 80s and is something people new to AI/ML often try their hand at building models for.\n",
    "\n",
    "###### About the data\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data\n",
    "\n",
    "This dataset consists of many pixelated digitized images of hand-written numbers.\n",
    "These images contain data in a single channel i.e. black and white images. White images on a Black background.\n",
    "\n",
    "\n",
    "We've loaded the mnist class and we just need to call `load_data` to get the dataset in order for us to use this data.\n",
    "\n",
    "This method returns 2 tuples. The training data and labels as well as the test data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12460d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Training and Test data\n",
    "tf.keras.datasets.mnist.load_data(\n",
    "    path='mnist.npz'\n",
    ")\n",
    "(X_train, Y_train), (X_test, Y_test) =  tf.keras.datasets.mnist.load_data()# FINISH ME #z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779095b-1a17-440a-853a-5009cd5816fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5782961-3c8f-4dd0-865f-9a8e277abc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0718970",
   "metadata": {},
   "source": [
    "# Visualize the dataset\n",
    "\n",
    "It's important to understand the data that you're trying to train on. In most cases preparing the data in some way that it's suitable for training is the most time consuming part of building and training a model to be useful.\n",
    "\n",
    "# Problem 1: Understanding the input data\n",
    "\n",
    "Now that we've loaded the data we need to make sure that it's in a format that we can use.\n",
    "\n",
    "## 1.1 First, What is the shape of an individual image?\n",
    "Firstly, what is the shape of the each input image?, and what range of values do the individual pixels within the image take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc8b30c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of each input image is : (28, 28)\n",
      "Number of images :(60000,)\n",
      "range of values of individual pixels :[0, 255]\n"
     ]
    }
   ],
   "source": [
    "print ('Shape of each input image is : {}'.format(X_train[0].shape))\n",
    "print('Number of images :{}'.format(X_train[:,0,0].shape))\n",
    "print('range of values of individual pixels :{}'.format([np.min(X_train), np.max(X_train)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b222e4",
   "metadata": {},
   "source": [
    "## 1.2 Plotting some of the input data\n",
    "\n",
    "Secondly, we should attempt to plot some of the input data to our fit to understand what the data looks like.\n",
    "It is possible to use the pyplot method to plot the images which we have just loaded into tensors.\n",
    "\n",
    "##### Hint:\n",
    "The options of `cmap='gray', interpolation='none'` with `pyplot.imshow` show the images as we would expect to see them.\n",
    "\n",
    "https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "\n",
    "#### Plot the Data\n",
    "\n",
    "###### Plot the first 2 values from the training portion of the dataset using the values to the first 2 training labels as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "477f41df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(X_test[0],cmap = 'grey', interpolation='none')  ## FINISH ME ##\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33af159f-3c4d-49c7-943a-5d5e2e28beea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "ax1.imshow(X_test[0],cmap = 'grey', interpolation='none')\n",
    "ax1.set_title(\"Plot 1\")\n",
    "ax2.imshow(X_test[1],cmap = 'grey', interpolation='none')\n",
    "ax2.set_title(\"Plot 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c9d2f-2c58-486d-a3d3-872fb3ea2b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dcb09b4",
   "metadata": {},
   "source": [
    "# Normalizing and Formatting the dataset\n",
    "\n",
    "Before we begin fitting the dataset it is useful to make sure that the data is in a format that we can use for fitting.\n",
    "\n",
    "\n",
    "\n",
    "## Problem 2: Formatting Input Data\n",
    "\n",
    "\n",
    "### 2.1 Normalize and reformat our Input Data.\n",
    "\n",
    "For our model we're going to use a flat array of 768 pixel values as our input and we want to make sure that our data has been correctly normalized so that the values are all in the unitary range [0,1].\n",
    "\n",
    "The data should be converted to the `'float32'` using the `astype` method and then normalized to have values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b5c39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# รท\\convert data into a float from an integer array\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)# FINISH ME #\n",
    "\n",
    "#normalizing the data to help with the training\n",
    "\n",
    "X_train = X_train/np.max(X_train)\n",
    "X_test = X_test/np.max(X_test)\n",
    "# FINISH ME #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1cb298b-6330-42c1-8701-76dff082a2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0855cae8-c235-45ef-95c4-9b1f5b504c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = tf.image.convert_image_dtype(X_train, tf.float32) #converts an unit8 image to a normlaised matrix with 0-1 \n",
    "X_test = tf.image.convert_image_dtype(X_test, tf.float32)\n",
    "np.max(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5dbcae",
   "metadata": {},
   "source": [
    "## 2.2 Resize the input data\n",
    "The input data should be re-formatted such that we have 1 flat array of inputs for each image.\n",
    "\n",
    "i.e. we have 60,000 images for training, we want each of these images to contain data in a flat array which means we are going from a 3D tensor to a 2D tensor. e.g. `(60000, 10, 10) ==> (60000, 100)` \n",
    "\n",
    "##### Hint: Tensor.Reshape can be used to simplify the re-shaping/re-sizing of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed89e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "## building the input 1D vector from the 28x28 pixels\n",
    "X_train = tf.reshape(X_train, (60000, 28*28))\n",
    "X_test =  tf.reshape(X_test, (10000, 28*28))\n",
    "# FINISH ME #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6255d932-20b8-49c6-b60d-53bf1185fa62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_image_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Convert `image` to `dtype`, scaling its values if needed.\n",
       "\n",
       "The operation supports data types (for `image` and `dtype`) of\n",
       "`uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`, `int64`,\n",
       "`float16`, `float32`, `float64`, `bfloat16`.\n",
       "\n",
       "Images that are represented using floating point values are expected to have\n",
       "values in the range [0,1). Image data stored in integer data types are\n",
       "expected to have values in the range `[0,MAX]`, where `MAX` is the largest\n",
       "positive representable number for the data type.\n",
       "\n",
       "This op converts between data types, scaling the values appropriately before\n",
       "casting.\n",
       "\n",
       "Usage Example:\n",
       "\n",
       ">>> x = [[[1, 2, 3], [4, 5, 6]],\n",
       "...      [[7, 8, 9], [10, 11, 12]]]\n",
       ">>> x_int8 = tf.convert_to_tensor(x, dtype=tf.int8)\n",
       ">>> tf.image.convert_image_dtype(x_int8, dtype=tf.float16, saturate=False)\n",
       "<tf.Tensor: shape=(2, 2, 3), dtype=float16, numpy=\n",
       "array([[[0.00787, 0.01575, 0.02362],\n",
       "        [0.0315 , 0.03937, 0.04724]],\n",
       "       [[0.0551 , 0.063  , 0.07086],\n",
       "        [0.07874, 0.0866 , 0.0945 ]]], dtype=float16)>\n",
       "\n",
       "Converting integer types to floating point types returns normalized floating\n",
       "point values in the range [0, 1); the values are normalized by the `MAX` value\n",
       "of the input dtype. Consider the following two examples:\n",
       "\n",
       ">>> a = [[[1], [2]], [[3], [4]]]\n",
       ">>> a_int8 = tf.convert_to_tensor(a, dtype=tf.int8)\n",
       ">>> tf.image.convert_image_dtype(a_int8, dtype=tf.float32)\n",
       "<tf.Tensor: shape=(2, 2, 1), dtype=float32, numpy=\n",
       "array([[[0.00787402],\n",
       "        [0.01574803]],\n",
       "       [[0.02362205],\n",
       "        [0.03149606]]], dtype=float32)>\n",
       "\n",
       ">>> a_int32 = tf.convert_to_tensor(a, dtype=tf.int32)\n",
       ">>> tf.image.convert_image_dtype(a_int32, dtype=tf.float32)\n",
       "<tf.Tensor: shape=(2, 2, 1), dtype=float32, numpy=\n",
       "array([[[4.6566129e-10],\n",
       "        [9.3132257e-10]],\n",
       "       [[1.3969839e-09],\n",
       "        [1.8626451e-09]]], dtype=float32)>\n",
       "\n",
       "Despite having identical values of `a` and output dtype of `float32`, the\n",
       "outputs differ due to the different input dtypes (`int8` vs. `int32`). This\n",
       "is, again, because the values are normalized by the `MAX` value of the input\n",
       "dtype.\n",
       "\n",
       "Note that converting floating point values to integer type may lose precision.\n",
       "In the example below, an image tensor `b` of dtype `float32` is converted to\n",
       "`int8` and back to `float32`. The final output, however, is different from\n",
       "the original input `b` due to precision loss.\n",
       "\n",
       ">>> b = [[[0.12], [0.34]], [[0.56], [0.78]]]\n",
       ">>> b_float32 = tf.convert_to_tensor(b, dtype=tf.float32)\n",
       ">>> b_int8 = tf.image.convert_image_dtype(b_float32, dtype=tf.int8)\n",
       ">>> tf.image.convert_image_dtype(b_int8, dtype=tf.float32)\n",
       "<tf.Tensor: shape=(2, 2, 1), dtype=float32, numpy=\n",
       "array([[[0.11811024],\n",
       "        [0.33858266]],\n",
       "       [[0.5590551 ],\n",
       "        [0.77952754]]], dtype=float32)>\n",
       "\n",
       "Scaling up from an integer type (input dtype) to another integer type (output\n",
       "dtype) will not map input dtype's `MAX` to output dtype's `MAX` but converting\n",
       "back and forth should result in no change. For example, as shown below, the\n",
       "`MAX` value of int8 (=127) is not mapped to the `MAX` value of int16 (=32,767)\n",
       "but, when scaled back, we get the same, original values of `c`.\n",
       "\n",
       ">>> c = [[[1], [2]], [[127], [127]]]\n",
       ">>> c_int8 = tf.convert_to_tensor(c, dtype=tf.int8)\n",
       ">>> c_int16 = tf.image.convert_image_dtype(c_int8, dtype=tf.int16)\n",
       ">>> print(c_int16)\n",
       "tf.Tensor(\n",
       "[[[  256]\n",
       "  [  512]]\n",
       " [[32512]\n",
       "  [32512]]], shape=(2, 2, 1), dtype=int16)\n",
       ">>> c_int8_back = tf.image.convert_image_dtype(c_int16, dtype=tf.int8)\n",
       ">>> print(c_int8_back)\n",
       "tf.Tensor(\n",
       "[[[  1]\n",
       "  [  2]]\n",
       " [[127]\n",
       "  [127]]], shape=(2, 2, 1), dtype=int8)\n",
       "\n",
       "Scaling down from an integer type to another integer type can be a lossy\n",
       "conversion. Notice in the example below that converting `int16` to `uint8` and\n",
       "back to `int16` has lost precision.\n",
       "\n",
       ">>> d = [[[1000], [2000]], [[3000], [4000]]]\n",
       ">>> d_int16 = tf.convert_to_tensor(d, dtype=tf.int16)\n",
       ">>> d_uint8 = tf.image.convert_image_dtype(d_int16, dtype=tf.uint8)\n",
       ">>> d_int16_back = tf.image.convert_image_dtype(d_uint8, dtype=tf.int16)\n",
       ">>> print(d_int16_back)\n",
       "tf.Tensor(\n",
       "[[[ 896]\n",
       "  [1920]]\n",
       " [[2944]\n",
       "  [3968]]], shape=(2, 2, 1), dtype=int16)\n",
       "\n",
       "Note that converting from floating point inputs to integer types may lead to\n",
       "over/underflow problems. Set saturate to `True` to avoid such problem in\n",
       "problematic conversions. If enabled, saturation will clip the output into the\n",
       "allowed range before performing a potentially dangerous cast (and only before\n",
       "performing such a cast, i.e., when casting from a floating point to an integer\n",
       "type, and when casting from a signed to an unsigned type; `saturate` has no\n",
       "effect on casts between floats, or on casts that increase the type's range).\n",
       "\n",
       "Args:\n",
       "  image: An image.\n",
       "  dtype: A `DType` to convert `image` to.\n",
       "  saturate: If `True`, clip the input before casting (if necessary).\n",
       "  name: A name for this operation (optional).\n",
       "\n",
       "Returns:\n",
       "  `image`, converted to `dtype`.\n",
       "\n",
       "Raises:\n",
       "  AttributeError: Raises an attribute error when dtype is neither\n",
       "  float nor integer.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/ops/image_ops_impl.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.image.convert_image_dtype?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77e7cb3d-c938-43f7-949d-cfe27b04205b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_reshaped' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_train_reshaped\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_reshaped' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909af7d-10bc-41ec-aeb8-adcbc1895f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69060a15",
   "metadata": {},
   "source": [
    "## 2.3 Change our input labels for fitting\n",
    "\n",
    "Now we need to make sure that the dataset labels are in a format which we can use alongside the training and testing dataset.\n",
    "\n",
    "Typically DNN expect category labels to be in a one-hot encoding. This means that labels are vectors with a single entry of 1 and all other entries being 0. \n",
    "\n",
    "##### Hint: This can be achieved using the `to_categorical` utility function in Keras.\n",
    "\n",
    "Using this change the Y_test and Y_train labels to be cateory labels.\n",
    "\n",
    "If you want to be explicit we know we're working with `n_classes = 10` due to looking at the numerical characters `0...9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5854c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "print(\"Shape before one-hot encoding: \", Y_train.shape)\n",
    "\n",
    "Y_train_cat = tf.keras.utils.to_categorical(Y_train, n_classes)\n",
    "Y_test_cat = tf.keras.utils.to_categorical(Y_test, n_classes) # FINISH ME #\n",
    "print(\"Shape after one-hot encoding: \", Y_train_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28bb7a",
   "metadata": {},
   "source": [
    "# Problem 3: Building the DNN\n",
    "\n",
    "\n",
    "In Tensorflow a 'standard' DNN model is defined using the Sequential model.\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
    "\n",
    "Then additional layers are added to the model one at a time until the model is complete using `model.add`.\n",
    "\n",
    "\n",
    "## Build a DNN with 2 \"hidden layers\"\n",
    "\n",
    "You should build a DNN such that it has 2 hidden and 1 output layer. When building a model we have neurons and activation functions. These are 'layers' in TensorFlow but we want 3 layers with neurons and activators such as: \n",
    "\n",
    "| <p align='left'> Layer |\n",
    "| :--- |\n",
    "| Dense |\n",
    "| Activation |\n",
    "| Dense |\n",
    "| Activation |\n",
    "| Dense |\n",
    "| Activation |\n",
    "\n",
    "A flat layer of neurons is defined using the `Dense` layer in Tensorflow.\n",
    "For our example we want to keep our Activators and our layers of neurons separate.\n",
    "    \n",
    "We want to use a random flat distribution of weights when initializing the parameters in this fit,\n",
    "    \n",
    "### !!Use the parameters `use_bias=True, kernel_initializer='RandomUniform'` when constructing a layer of neurons!!\n",
    "\n",
    "###### NB: By default each Dense layer also applies a bias to each node to improve the fit performance.\n",
    "    \n",
    "For our hidden nodes we want to use the `'GELU'` Activator for this example and `'softmax'` to build our a numerical classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba268e86",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m weight_start_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomUniform\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# building a linear stack of layers with the sequential model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m()\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(n_nodes, activation \u001b[38;5;241m=\u001b[39m activation_type,use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomUniform\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m## FINISH ME ## ))\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(n_nodes, activation \u001b[38;5;241m=\u001b[39m activation_type,use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomUniform\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m## FINISH ME ## ))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "n_categories = 10\n",
    "n_nodes = 256\n",
    "activation_type = 'gelu'\n",
    "weight_start_values = 'RandomUniform'\n",
    "\n",
    "# building a linear stack of layers with the sequential model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(n_nodes, activation = activation_type,use_bias=True, kernel_initializer='RandomUniform')) ## FINISH ME ## ))\n",
    "model.add(Dense(n_nodes, activation = activation_type,use_bias=True, kernel_initializer='RandomUniform')) ## FINISH ME ## ))\n",
    "model.add(Dense(n_categories,use_bias=True, kernel_initializer='RandomUniform', activation = 'softmax'))\n",
    "\n",
    "### FINISH ME ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d8746-46fe-43fa-b18f-347cb9e33a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e825c92",
   "metadata": {},
   "source": [
    "## Compiling our model\n",
    "\n",
    "At this point we have a description of our model. Now we want to compile it.\n",
    "\n",
    "Strictly speaking some of the steps here are optional, but we want to be explicit with how we're building our model to be sure that we've understood everything that is going on.\n",
    "\n",
    "We will be using `categorical_crossentropy` as discussed in our lecture as our loss model.\n",
    "\n",
    "We will also be using the `Stochastic Gradient Descent` or `SGD` optimizer for our training. This is different to the default.\n",
    "\n",
    "`metrics=['accuracy']` is also passed to our model to ensure that it collects training data that we're interested in later on.\n",
    "\n",
    "\n",
    "## Now compile and build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3deac8-536b-4262-93a6-0479dc1bb53e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='SGD')\n",
    "# Now we want to 'build' the model, so tell it about our input data format\n",
    "model.build(input_shape=(1,784))\n",
    "\n",
    "# Print a helpful summary of our model\n",
    "model.summary()\n",
    "\n",
    "# Now we're going to take a copy of the weights in our first hidden layer of the fit\n",
    "weights_before_training = model.layers[0].get_weights()[0].copy()\n",
    "weights_before_training.resize(200704)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3ef3d",
   "metadata": {},
   "source": [
    "#### Problem 4: Understanding our model\n",
    "\n",
    "## Q: 4.1\n",
    "\n",
    "##### Using an Input Shape of `784` and a hidden layer of `256` nodes you should see `200960` free parameters in the first layer of your model.\n",
    "\n",
    "##### Explain where this number of free parameters comes from.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee2c65",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "## This is because the number of weights are 784x256 and biases are 256. Total parameters = 784x256 + 256 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad93994",
   "metadata": {},
   "source": [
    "## Q: 4.2\n",
    "\n",
    "##### Using `256` nodes per hidden layer there will also be `65792` weights between the 2 hidden layers in the model.\n",
    "\n",
    "##### Explain where these weights come from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd4150",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "### 256x256 + 256 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47c4f2",
   "metadata": {},
   "source": [
    "## Q: 4.3\n",
    "\n",
    "##### Explain why a `'softmax'` activation function is used at the end of our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735055f0",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "\n",
    "### because softmax provides a normalised output contrary to GELU or ReLU ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3c156",
   "metadata": {},
   "source": [
    "## Q: 4.4\n",
    "\n",
    "##### How many weights would we expect in the whole model when using 128 neurons per hidden layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8486e255",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "###  118282 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d623e92",
   "metadata": {},
   "source": [
    "## Q: 4.5\n",
    "\n",
    "##### Why are activation functions needed in a DNN which is trained on data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846da7a7",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "### To normalise output to desired values, to deal with non-linear relationships ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d1791",
   "metadata": {},
   "source": [
    "\n",
    "# Problem 5: Train the model\n",
    "\n",
    "### Model Training\n",
    "\n",
    "Now that we have a model we want to fit we now want to train it on our training dataset making sure to use our validation data to check on our training.\n",
    "\n",
    "The result of this training will be stored in our history and will allow us to go back and asses how well our training worked.\n",
    "\n",
    "\n",
    "## 5.1 Perform the training\n",
    "Complete and run the command below to train the model on data using a `batch_size` of 600 for 100 `epochs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c7124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training the model and saving metrics in history\n",
    "history = model.fit(X_train, Y_train_cat, validation_data=(X_test, Y_test_cat), verbose=2,\n",
    "          batch_size=  600, epochs=100)\n",
    "\n",
    "## Now take a copy of the weights in the first hidden layer _after_ training on data\n",
    "weights_after_training = model.layers[0].get_weights()[0].copy()\n",
    "weights_after_training.resize(200704)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff2f3ab-954f-4e2e-9d02-efaa36ca27c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f49b609",
   "metadata": {},
   "source": [
    "## Q: 5.2\n",
    "\n",
    "##### Why are there 100 steps per epoc in this training? (0.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849846dd",
   "metadata": {},
   "source": [
    "### Answer: The model uses the same data 100 times (number of epochs). In one epoch, model train on batch_size at one instance, thus to trin on the whole dataset, each epoch has (len(data)/batch_size) number of steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab5c0a",
   "metadata": {},
   "source": [
    "\n",
    "# Problem6: Understanding the training history\n",
    "\n",
    "### Cross-Checking our Model fit\n",
    "\n",
    "The returned history from our fit contains both the accuracy and the loss function over the epocs of training.\n",
    "\n",
    "This information can be accessed via: `history.history[]`\n",
    "\n",
    "We now want to make 2 plots:\n",
    "\n",
    "\n",
    "## 6.1 Plotting Evolution of model Accuracy\n",
    "\n",
    "This plot will show how the accuracy of the model has increased vs epocs of training.\n",
    "\n",
    "This should show for a good model that accuracy increases with the number of epocs.\n",
    "\n",
    "###### Hint:\n",
    "`history.history[accuracy]` vs `history.history[val_accuracy]` give the model accuracy during the training epocs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af2d398-d955-47db-8098-9d6a85be072b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_training = history.history['accuracy']\n",
    "accuracy_test = history.history['val_accuracy']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(accuracy_training, accuracy_test)\n",
    "ax.set_xlabel('accuracy')\n",
    "ax.set_ylabel('val_accuracy')\n",
    "plt.show()\n",
    "## FINISH ME ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3e7ac",
   "metadata": {},
   "source": [
    "## 6.2 Plotting Evolution of model losses\n",
    "\n",
    "As above, but now we want to see how the loss function evolves using the `loss` vs `val_loss` parameters vs epoc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a49dc-73b9-4c5e-9a81-4e0e0dd73975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(training_loss, test_loss)\n",
    "ax.set_xlabel('training loss')\n",
    "ax.set_ylabel('val_loss')\n",
    "plt.show()\n",
    "## FINISH ME ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf627c",
   "metadata": {},
   "source": [
    "## Q 6.3: Is this model over-training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76966f52",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "No, model over trains if the accuracy is higher than val_accuracy\n",
    "### FINISH ME ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894d0cc",
   "metadata": {},
   "source": [
    "\n",
    "# Problem 7: Making predictions\n",
    "\n",
    "### Testing how well our model classifies different numbers\n",
    "\n",
    "In order to test how well our model actually classifies different numbers we need to be able to make predictions using it.\n",
    "\n",
    "Our model is designed to output the probability of each label being true (according to the model).\n",
    "\n",
    "With that in mind we want to get the best value from a single image.\n",
    "\n",
    "\n",
    "Our model has been defined to take an input of shape `(1, 784)`. This allows us to pass many images at a time to the model to either train or to have predictions made on.\n",
    "\n",
    "\n",
    "## 7.1 Predicting with our model\n",
    "\n",
    "### Want to make a prediction using the 10th element of the testing dataset\n",
    "\n",
    "Lets first make a single prediction using our dataset and model.\n",
    "\n",
    "To do this we need to:\n",
    "\n",
    "1. Take the 10th element from our test dataset\n",
    "2. Resize this to match the expected input dimensions from our model\n",
    "3. Make a prediction using this input using `model.predict`\n",
    "4. Make a bar chart showing the predictions from the model.\n",
    "\n",
    "###### Hint: `plt.bar` is useful for plotting the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca3e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## FINISH ME ##\n",
    "# tenth_element = X_test[10]\n",
    "# tenth_element.shape\n",
    "output = model.predict(X_train[10:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ce533-dd57-4e65-9fb9-960a23af9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.flatten()\n",
    "classes = np.arange(len(output))\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(classes, output)\n",
    "plt.xticks(classes) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c132c2",
   "metadata": {},
   "source": [
    "## 7.2 Making a Numerical Prediction\n",
    "\n",
    "The method `argmax` from numpy can help us change this prediction distribution from a vector to a single value.\n",
    "\n",
    "### Predict the number in this image ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858db180",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FINISH ME ##\n",
    "np.argmax(model.predict(X_train[10:11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265939e0",
   "metadata": {},
   "source": [
    "# Problem 8 Testing the model\n",
    "\n",
    "One of the ways we can evaluate how well our model is spearating our input data we can build a confusion matrix.\n",
    "\n",
    "This allows us to see how many numbers are incorrectly identified using a labelled dataset.\n",
    "\n",
    "## Constructing a Confusion Matrix\n",
    "\n",
    "In order to construct a confusion matrix we need to make predictions over a large number of inputs and be able to compare them to labels which are known to be correct.\n",
    "\n",
    "Taking the 'training' dataset can collect numerical predictions using `argmax` for each of the 60,000 entries in the dataset.\n",
    "\n",
    "## 8.1 Make many predictions\n",
    "\n",
    "To collect enough data to produce our confusion matrix we can make predictions over the whole training (or test) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deac50d-7f8e-4c3a-94d2-fbe62fa78aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = model.predict(X_test)\n",
    "pred_ref = []\n",
    "for i, row in enumerate(predictions):\n",
    "    pred_ref.append(np.argmax(predictions[i]))\n",
    "#pred_ref = np.argmax(pred)\n",
    "## FINISH ME ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd4596-7733-4d4b-aa3a-5e967c786257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4cff0da",
   "metadata": {},
   "source": [
    "## 8.2 Plot Confusion Matrix\n",
    "\n",
    "We can now construct a confusion matrix using `tf.math.confusion_matrix` and plot it with `sns.heatmap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da9e4b-c0a1-470e-841f-e557b2e61d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c9f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the confusion matrix\n",
    "confusion_matrix = tf.math.confusion_matrix(Y_test, pred_ref) ## FINISH ME ## )\n",
    "\n",
    "# Import the useful tools for plotting a confusion matrix\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Plot the confusion matrix using a heatmap\n",
    "sns.heatmap(confusion_matrix, annot=False, cmap='Blues', norm=LogNorm())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dfdbd9",
   "metadata": {},
   "source": [
    "# 9: Making Predictions with new data\n",
    "\n",
    "Make aure that you download the sample image \n",
    "\n",
    "## 9.1 Load data from `sample_image.png`\n",
    "\n",
    "Complete the load_image method below to load a new numerical image file from disk, resize and normalize the data into a format which we can use with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfdd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare the image\n",
    "def load_image(filename):\n",
    " # load the image\n",
    " #img = load_img(filename, target_size=(28, 28), color_mode='grayscale')\n",
    " img = load_img(filename, target_size=(28, 28), color_mode='grayscale')\n",
    " # convert to array\n",
    " img = img_to_array(img)\n",
    " \n",
    " ## FINISH ME ##\n",
    " ## NEED TO RESIZE AND NORMALIZE INPUT FOR MODEL ## \n",
    " return img\n",
    "\n",
    "# load the image\n",
    "img = load_image('./sample_image.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a54bae-d2b5-4745-a978-9b5064452238",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_reshaped = img.reshape(1,28*28)\n",
    "img_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18bf6ba",
   "metadata": {},
   "source": [
    "## 9.2 Make a prediction on the new data\n",
    "\n",
    "Now that we've loaded the sample file, use our model to make a prediction on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the class\n",
    "predict_value = model.predict(img_reshaped)\n",
    "predict_value_digit = argmax(predict_value)\n",
    "print('Prediction: {}'.format(predict_value_digit))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(mpimg.imread('./sample_image.png'), cmap='gray', interpolation='none')\n",
    "plt.title(\"Sample File\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(2,1,2)\n",
    "plt.bar(np.arange(10), predict_value[0])\n",
    "## FINISH ME ##\n",
    "## WANT TO PLOT DISTRIBUTION OF PREDICTIONS ##\n",
    "plt.yscale('log')\n",
    "plt.title(\"Predicted Value Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7777218-738a-4765-8b17-efbe756bcc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.yscale('log')\n",
    "# plt.title(\"Predicted Value Distribution\")\n",
    "predict_value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619684a",
   "metadata": {},
   "source": [
    "# Bonus 1: Analyzing Weights\n",
    "\n",
    "Before and after fitting our model we took copies of the weights used in training our dataset.\n",
    "\n",
    "If you construct a histogram of the weights from before fitting what distribution does it follow?\n",
    "\n",
    "## B1.1 Plot weights from before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319e643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(weights_before_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e19657",
   "metadata": {},
   "source": [
    "## B1.2 Plot the weights After training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ccd256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(weights_after_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c5ec28",
   "metadata": {},
   "source": [
    "## B1.3 Compare the weights\n",
    "\n",
    "Construct a plot comparing the weights before fitting to the values after fitting.\n",
    "\n",
    "##### Hint: You may need to use a log plot in y to see the full distribution(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba82be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(weights_after_training)\n",
    "plt.hist(weights_before_training)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02b2d4",
   "metadata": {},
   "source": [
    "## B1.4 Q: How might the weights vary if you train the model for an additional 80 steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b75f3b",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "A sharper bell curve centererd at 0\n",
    "### FINISH ME ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cab702",
   "metadata": {},
   "source": [
    "## B1.5 Q: How would inclding a regularizer impact the distribution of weights after training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d45f7",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "will reduce the tail fatness\n",
    "### FINISH ME ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59248c16",
   "metadata": {},
   "source": [
    "# Bonus 2: Loading a model and optimizing it further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8c458",
   "metadata": {},
   "source": [
    "## B2.1 Saving our model\n",
    "\n",
    "Lets save our model to disk so we can take it home and use it to identify numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a8476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model_path = './keras_mnist.keras'\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424cc24",
   "metadata": {},
   "source": [
    "## B2.2 Lets load our model and do some further training\n",
    "\n",
    "We can load a keras model from disk using the `'load_model'` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d1c9f7-6ad6-44e8-9ce2-593995954bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโณโโโโโโโโโโโโโโโโโโโโโโโโโณโโโโโโโโโโโโโโโโ\n",
       "โ<span style=\"font-weight: bold\"> Layer (type)                    </span>โ<span style=\"font-weight: bold\"> Output Shape           </span>โ<span style=\"font-weight: bold\">       Param # </span>โ\n",
       "โกโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโฉ\n",
       "โ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   โ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               โ       <span style=\"color: #00af00; text-decoration-color: #00af00\">200,960</span> โ\n",
       "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโผโโโโโโโโโโโโโโโโโโโโโโโโโผโโโโโโโโโโโโโโโโค\n",
       "โ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 โ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               โ        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> โ\n",
       "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโผโโโโโโโโโโโโโโโโโโโโโโโโโผโโโโโโโโโโโโโโโโค\n",
       "โ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 โ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                โ         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> โ\n",
       "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโดโโโโโโโโโโโโโโโโโโโโโโโโโดโโโโโโโโโโโโโโโโ\n",
       "</pre>\n"
      ],
      "text/plain": [
       "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโณโโโโโโโโโโโโโโโโโโโโโโโโโณโโโโโโโโโโโโโโโโ\n",
       "โ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mโ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mโ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mโ\n",
       "โกโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโฉ\n",
       "โ dense (\u001b[38;5;33mDense\u001b[0m)                   โ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)               โ       \u001b[38;5;34m200,960\u001b[0m โ\n",
       "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโผโโโโโโโโโโโโโโโโโโโโโโโโโผโโโโโโโโโโโโโโโโค\n",
       "โ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 โ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)               โ        \u001b[38;5;34m65,792\u001b[0m โ\n",
       "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโผโโโโโโโโโโโโโโโโโโโโโโโโโผโโโโโโโโโโโโโโโโค\n",
       "โ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 โ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)                โ         \u001b[38;5;34m2,570\u001b[0m โ\n",
       "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโดโโโโโโโโโโโโโโโโโโโโโโโโโดโโโโโโโโโโโโโโโโ\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,324</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m269,324\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,322</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m269,322\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "nu_model = load_model('./keras_mnist.keras')\n",
    "nu_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227e1c59",
   "metadata": {},
   "source": [
    "## B2.3 Make some small changes this time\n",
    "\n",
    "Now we want to try training using the `'Adam'` optimizer so re-compile the model.\n",
    "\n",
    "(If you re-run model.summary you will find it doesn't need to be re-built)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the sequential model\n",
    "nu_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='Adam')\n",
    "\n",
    "# Print a helpful summary of our model\n",
    "nu_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88cd17",
   "metadata": {},
   "source": [
    "## B2.4 Re-Train\n",
    "\n",
    "We don't have new data to train our model on, but what happens if we continue training with our existing dataset?\n",
    "\n",
    "Re-train as before with the same data and batch/epoch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model and saving metrics in history\n",
    "nu_history = nu_model.fit(X_train, Y_train_cat, validation_data=(X_test, Y_test_cat), verbose=2,\n",
    "          batch_size=600,\n",
    "          epochs=30\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1290e",
   "metadata": {},
   "source": [
    "## B2.5 Evaluating Extended Model Accuracy\n",
    "\n",
    "Lets make another set of plots to compare how our model accuracy improved with additional training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f3c26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_training = nu_history.history['accuracy']\n",
    "accuracy_test = nu_history.history['val_accuracy']\n",
    "\n",
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "plt.plot(accuracy_training)\n",
    "plt.plot(accuracy_test)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de963f38",
   "metadata": {},
   "source": [
    "## B2.6 Compare Extended loss functions\n",
    "\n",
    "As above lets generate some new plots for our loss function evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac801c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = nu_history.history['loss']\n",
    "validation_loss = nu_history.history['val_loss']\n",
    "\n",
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "plt.plot(training_loss)\n",
    "plt.plot(validation_loss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4acf560",
   "metadata": {},
   "source": [
    "## B2.7 Have we over-trained?\n",
    "\n",
    "Using the above plots do you think we've now over-trained our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6f3cd",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "Yes ! As the loss function on validation data is increasing after epoch number 5. And accuracy of the validation data remains static while that of the test data increases.\n",
    "### FINISH ME ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e9452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
